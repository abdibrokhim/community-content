---
title: "LLaVA"
description: "A state-of-the-art multimodal language and vision model achieving impressive chat capabilities and setting new accuracy benchmarks."
---

# LLaVA: Large Language and Vision Assistant

LLaVA represents a novel end-to-end trained large multimodal model that combines a vision encoder and Vicuna for general-purpose visual and language understanding, achieving impressive chat capabilities mimicking spirits of the multimodal GPT-4 and setting a new state-of-the-art accuracy on Science QA.

| General  |  |
| --- | --- |
| Relese date |  November 20, 2023 |
| Repository | https://github.com/haotian-liu/LLaVA |
| Type | Multimodal Language and Vision Model |

## What is LLaVA?

Visual Instruction Tuning: LLaVA, short for Large Language-and-Vision Assistant, represents a significant leap in multimodal AI models. 

With a focus on visual instruction tuning, LLaVA has been engineered to rival the capabilities of GPT-4V, demonstrating its exceptional prowess in understanding both language and vision. This state-of-the-art model excels in tasks ranging from impressive chatbot interactions to setting a new standard in science question-answering accuracy, achieving a remarkable 92.53%. With LLaVA's innovative approach to instruction-following data and the effective combination of vision and language models, it promises a versatile solution for diverse applications, marking a significant milestone in the field of multimodal AI.

### LLaVA Tutorials
<TechTutorials/>
---

### LLaVA Libraries
A curated list of libraries and technologies to help you build great projects with 'technology'.

* [LLaVA Research Paper](https://llava-vl.github.io)
* [GitHub Repo](https://github.com/haotian-liu/LLaVA)
* [LLaVA Demo](https://llava.hliu.cc)
* [Model Zoo](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)

---
